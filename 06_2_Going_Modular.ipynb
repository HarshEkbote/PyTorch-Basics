{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Going Modular using Python Scripts\n",
        "\n",
        "In this notebook we use python scripts to get data, transform, train, evaluate and save the model using one command in the terminal."
      ],
      "metadata": {
        "id": "Djt2qkOvk4r6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get data"
      ],
      "metadata": {
        "id": "A-oc7bGYlLfp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-dCAYT5kVGB",
        "outputId": "16b13584-61ad-41aa-ba1e-435b705641c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Directory data/pizza_steak_sushi_mod already exists!\n",
            "Dataset downloaded\n",
            "Unzipped the zip file!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import zipfile\n",
        "import requests\n",
        "from pathlib import Path\n",
        "\n",
        "data_path=Path('data/')\n",
        "image_path=data_path/'pizza_steak_sushi_mod'\n",
        "\n",
        "if image_path.is_dir():\n",
        "  print(f\"Directory {image_path} already exists!\")\n",
        "else:\n",
        "  image_path.mkdir(parents=True,exist_ok=True)\n",
        "  print(\"Directory Created!\")\n",
        "\n",
        "with open(data_path/'pizza_steak_sushi.zip','wb') as f:\n",
        "  request=requests.get('https://github.com/HarshEkbote/PyTorch-Basics/raw/main/data/pizza_steak_sushi.zip')\n",
        "  f.write(request.content)\n",
        "  print(\"Dataset downloaded\")\n",
        "\n",
        "with zipfile.ZipFile(data_path/'pizza_steak_sushi.zip','r') as zipref:\n",
        "  zipref.extractall(image_path)\n",
        "  print(\"Unzipped the zip file!\")\n",
        "\n",
        "os.remove(data_path/'pizza_steak_sushi.zip')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Creating Dataset and DataLoaders in script mode\n"
      ],
      "metadata": {
        "id": "IAPkST60mrZ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  os.mkdir('going_modular')\n",
        "except:\n",
        "  print(\"Already exists\")"
      ],
      "metadata": {
        "id": "HyBJ02IVoiow",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bca88458-681d-4032-b399-d997085a4373"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Already exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile going_modular/data_setup.py\n",
        "\"Contains the functionality for craeting Pytorch dataloaders for image classification data\"\n",
        "\n",
        "import os\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms,datasets\n",
        "\n",
        "NUM_WORK=os.cpu_count()\n",
        "\n",
        "def create_dataloader(train_dir:str,test_dir:str,transform:transforms.Compose,batch_size:int,num_workers:int=NUM_WORK):\n",
        "  train_data=datasets.ImageFolder(train_dir,transform=transform)\n",
        "  test_data=datasets.ImageFolder(test_dir,transform=transform)\n",
        "\n",
        "  class_name=train_data.classes\n",
        "\n",
        "  train_dataloader=DataLoader(\n",
        "      train_data,\n",
        "      batch_size=batch_size,\n",
        "      shuffle=True,\n",
        "      num_workers=num_workers,\n",
        "      pin_memory=True\n",
        "  )\n",
        "\n",
        "  test_dataloader=DataLoader(\n",
        "      test_data,\n",
        "      batch_size=batch_size,\n",
        "      shuffle=False,\n",
        "      num_workers=num_workers,\n",
        "      pin_memory=True\n",
        "  )\n",
        "\n",
        "  return train_dataloader,test_dataloader,class_name"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBzCcuD1mlwq",
        "outputId": "c84f41f9-80de-46cb-9e0c-fab0d236a753"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting going_modular/data_setup.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creting the model in script mode"
      ],
      "metadata": {
        "id": "VVykMpSJos9Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile going_modular/model_builder.py\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class TinyVGG(nn.Module):\n",
        "  def __init__(self,input_shape:int,hidden_units:int,output_shape:int):\n",
        "    super().__init__()\n",
        "    self.block_1 = nn.Sequential(\n",
        "          nn.Conv2d(in_channels=input_shape,\n",
        "                    out_channels=hidden_units,\n",
        "                    kernel_size=3,\n",
        "                    stride=1,\n",
        "                    padding=0),\n",
        "          nn.ReLU(),\n",
        "          nn.Conv2d(in_channels=hidden_units,\n",
        "                    out_channels=hidden_units,\n",
        "                    kernel_size=3,\n",
        "                    stride=1,\n",
        "                    padding=0),\n",
        "          nn.ReLU(),\n",
        "          nn.MaxPool2d(kernel_size=2,\n",
        "                        stride=2)\n",
        "      )\n",
        "    self.block_2 = nn.Sequential(\n",
        "          nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=0),\n",
        "          nn.ReLU(),\n",
        "          nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=0),\n",
        "          nn.ReLU(),\n",
        "          nn.MaxPool2d(2)\n",
        "        )\n",
        "    self.classifier = nn.Sequential(\n",
        "          nn.Flatten(),\n",
        "          nn.Linear(in_features=hidden_units*13*13,\n",
        "                    out_features=output_shape)\n",
        "        )\n",
        "  def forward(self, x: torch.Tensor):\n",
        "        # x = self.conv_block_1(x)\n",
        "        # x = self.conv_block_2(x)\n",
        "        # x = self.classifier(x)\n",
        "        # return x\n",
        "        return self.classifier(self.block_2(self.block_1(x)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pfOmPgVHocGr",
        "outputId": "23c1461d-8cdd-4df3-c44c-d8d1b7e58831"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting going_modular/model_builder.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from going_modular import model_builder\n",
        "device='cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "torch.manual_seed(42)\n",
        "model0=model_builder.TinyVGG(input_shape=2,hidden_units=10,output_shape=3).to(device)\n",
        "\n",
        "model0"
      ],
      "metadata": {
        "id": "mpO2ePWdqFWo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de231e4e-ebbe-4879-ee0f-62c46000d6a4"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TinyVGG(\n",
              "  (conv_block_1): Sequential(\n",
              "    (0): Conv2d(2, 10, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (1): ReLU()\n",
              "    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (3): ReLU()\n",
              "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (conv_block_2): Sequential(\n",
              "    (0): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (1): ReLU()\n",
              "    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (3): ReLU()\n",
              "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (classifier): Sequential(\n",
              "    (0): Flatten(start_dim=1, end_dim=-1)\n",
              "    (1): Linear(in_features=1690, out_features=3, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train loop, Test Loop and Training the model"
      ],
      "metadata": {
        "id": "PTMwhYqPet2v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile going_modular/engine.py\n",
        "\n",
        "from typing import Tuple,Dict,List\n",
        "import torch\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "def train_step(model:torch.nn.Module,dataloader:torch.utils.data.DataLoader,loss_fn:torch.nn.Module,optimizer:torch.optim.Optimizer,device:torch.device)->Tuple[float,float]:\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  train_loss,train_acc=0,0\n",
        "\n",
        "  for batch,(x,y) in enumerate(dataloader):\n",
        "    #print(x.dtype)\n",
        "    x,y=x.to(device),y.to(device)\n",
        "\n",
        "    y_pred=model(x)\n",
        "\n",
        "    loss=loss_fn(y_pred,y)\n",
        "    train_loss+=loss.item()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    y_pred_class=torch.argmax(torch.softmax(y_pred,dim=1),dim=1)\n",
        "    train_acc+=(y_pred_class==y).sum().item()/len(y_pred)\n",
        "\n",
        "  train_loss=train_loss/len(dataloader)\n",
        "  train_acc=train_acc/len(dataloader)\n",
        "  return train_loss,train_acc\n",
        "\n",
        "def test_step(model:torch.nn.Module,dataloader:torch.utils.data.DataLoader,loss_fn:torch.nn.Module,device:torch.device)->Tuple[float,float]:\n",
        "  model.eval()\n",
        "\n",
        "  test_loss,test_acc=0,0\n",
        "  with torch.inference_mode():\n",
        "    for batch,(x,y) in enumerate(dataloader):\n",
        "      x,y=x.to(device),y.to(device)\n",
        "\n",
        "      test_pred_logits=model(x)\n",
        "\n",
        "      loss=loss_fn(test_pred_logits,y)\n",
        "      test_loss+=loss.item()\n",
        "\n",
        "      test_pred_label=test_pred_logits.argmax(dim=1)\n",
        "      test_acc+=((test_pred_label==y).sum().item()/len(test_pred_label))\n",
        "\n",
        "  test_loss=test_loss/len(dataloader)\n",
        "  test_acc=test_acc/len(dataloader)\n",
        "  return test_loss,test_acc\n",
        "\n",
        "def train(model:torch.nn.Module,train_dataloader:torch.utils.data.DataLoader,test_dataloader:torch.utils.data.DataLoader,loss_fn:torch.nn.Module,optimizer:torch.optim.Optimizer,epochs:int,device:torch.device)->Dict[str,List]:\n",
        "  results={\n",
        "      'train_loss':[],\n",
        "      'train_acc':[],\n",
        "      'test_loss':[],\n",
        "      'test_acc':[]\n",
        "  }\n",
        "\n",
        "  for epoch in tqdm(range(epochs)):\n",
        "    train_loss,train_acc=train_step(model,train_dataloader,loss_fn,optimizer,device)\n",
        "    test_loss,test_acc=test_step(model,test_dataloader,loss_fn,device)\n",
        "\n",
        "    print(\n",
        "        f\"Epoch: {epoch+1} | \"\n",
        "        f\"train_loss: {train_loss:.4f} |\"\n",
        "        f'train_acc: {train_acc:.2f} | '\n",
        "        f'test_loss: {test_loss:.4f} | '\n",
        "        f'test_acc: {test_acc:.2f}'\n",
        "    )\n",
        "\n",
        "    results['train_loss'].append(train_loss)\n",
        "    results['train_acc'].append(train_acc)\n",
        "    results['test_loss'].append(test_loss)\n",
        "    results['test_acc'].append(test_acc)\n",
        "  return results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-nrWcu_9c-qz",
        "outputId": "18f839de-e0e0-40b0-ef11-24b93f5ad740"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting going_modular/engine.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Script to save the model"
      ],
      "metadata": {
        "id": "4UhKVtBfjd5b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile going_modular/utils.py\n",
        "\n",
        "from pathlib import Path\n",
        "import torch\n",
        "\n",
        "def save_model(model:torch.nn.Module,target_dir:str,model_name:str):\n",
        "  target_dir_path=Path(target_dir)\n",
        "  target_dir_path.mkdir(parents=True,exist_ok=True)\n",
        "\n",
        "  assert model_name.endswith(\".pth\") or model_name.endswith('.pt'), \"Must end with .pt or .pth\"\n",
        "  model_save_path=target_dir_path/model_name\n",
        "\n",
        "  print(f\"[INFO] model saving to {model_save_path}\")\n",
        "  torch.save(obj=model.state_dict(),f=model_save_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HymKJvbqiw2J",
        "outputId": "c4ba6591-2a9c-4918-9995-e771298eaf9e"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting going_modular/utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training the model"
      ],
      "metadata": {
        "id": "bcenX7NJlM1x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile going_modular/train.py\n",
        "\n",
        "import os\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "import data_setup,engine,model_builder,utils\n",
        "\n",
        "NUM_EPOCHS=10\n",
        "BATCH_SIZE=32\n",
        "HIDDEN_UNITS=10\n",
        "LRATE=0.01\n",
        "\n",
        "train_dir='data/pizza_steak_sushi_mod/train'\n",
        "test_dir='data/pizza_steak_sushi_mod/test'\n",
        "\n",
        "device= 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "data_transform=transforms.Compose([\n",
        "    transforms.Resize(size=(64,64)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "train_dataloader,test_dataloader,class_names=data_setup.create_dataloader(\n",
        "    train_dir=train_dir,\n",
        "    test_dir=test_dir,\n",
        "    transform=data_transform,\n",
        "    batch_size=BATCH_SIZE\n",
        ")\n",
        "\n",
        "model=model_builder.TinyVGG(\n",
        "    input_shape=3,\n",
        "    hidden_units=HIDDEN_UNITS,\n",
        "    output_shape=len(class_names)).to(device)\n",
        "\n",
        "loss_fn=torch.nn.CrossEntropyLoss()\n",
        "optimizer=torch.optim.Adam(params=model.parameters(),lr=LRATE)\n",
        "\n",
        "engine.train(model=model,\n",
        "             train_dataloader=train_dataloader,\n",
        "             test_dataloader=test_dataloader,\n",
        "             loss_fn=loss_fn,\n",
        "             optimizer=optimizer,\n",
        "             epochs=NUM_EPOCHS,\n",
        "             device=device)\n",
        "\n",
        "utils.save_model(model=model,\n",
        "                 target_dir='models',\n",
        "                 model_name='script_based_model.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T25o7uDxk6Hf",
        "outputId": "6d15ea1f-4320-4357-8be2-6b6b683415a2"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting going_modular/train.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python going_modular/train.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EOU9dRHhnKNG",
        "outputId": "8ca4efb4-958f-42ef-d055-62f7bc819b0f"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  0% 0/10 [00:00<?, ?it/s]Epoch: 1 | train_loss: 1.1468 |train_acc: 0.28 | test_loss: 1.0923 | test_acc: 0.54\n",
            " 10% 1/10 [00:01<00:16,  1.78s/it]Epoch: 2 | train_loss: 1.0993 |train_acc: 0.41 | test_loss: 1.0998 | test_acc: 0.20\n",
            " 20% 2/10 [00:03<00:14,  1.78s/it]Epoch: 3 | train_loss: 1.1005 |train_acc: 0.29 | test_loss: 1.1042 | test_acc: 0.20\n",
            " 30% 3/10 [00:05<00:12,  1.77s/it]Epoch: 4 | train_loss: 1.0937 |train_acc: 0.41 | test_loss: 1.1017 | test_acc: 0.20\n",
            " 40% 4/10 [00:07<00:11,  1.87s/it]Epoch: 5 | train_loss: 1.1012 |train_acc: 0.29 | test_loss: 1.1041 | test_acc: 0.20\n",
            " 50% 5/10 [00:10<00:12,  2.41s/it]Epoch: 6 | train_loss: 1.0938 |train_acc: 0.41 | test_loss: 1.1011 | test_acc: 0.20\n",
            " 60% 6/10 [00:12<00:08,  2.21s/it]Epoch: 7 | train_loss: 1.1011 |train_acc: 0.29 | test_loss: 1.1032 | test_acc: 0.20\n",
            " 70% 7/10 [00:14<00:06,  2.07s/it]Epoch: 8 | train_loss: 1.1105 |train_acc: 0.29 | test_loss: 1.0997 | test_acc: 0.20\n",
            " 80% 8/10 [00:16<00:03,  1.98s/it]Epoch: 9 | train_loss: 1.1066 |train_acc: 0.29 | test_loss: 1.1004 | test_acc: 0.20\n",
            " 90% 9/10 [00:17<00:01,  1.92s/it]Epoch: 10 | train_loss: 1.1006 |train_acc: 0.29 | test_loss: 1.1039 | test_acc: 0.20\n",
            "100% 10/10 [00:19<00:00,  1.97s/it]\n",
            "[INFO] model saving to models/script_based_model.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RLm_k5DxnQVo"
      },
      "execution_count": 94,
      "outputs": []
    }
  ]
}